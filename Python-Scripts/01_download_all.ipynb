{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAGES_C-PEAT Data Retrieval and Event Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### PANGAEApy\n",
    "## if you need to install PANGAEApy use pip\n",
    "#!pip install pangaeapy # Uncomment to install pangaeapy\n",
    "\n",
    "## if you need to upgrade PANGAEApy use \n",
    "#!pip install pangaeapy --upgrade # Uncomment to upgrade pangaeapy\n",
    "\n",
    "## check version of PANGAEApy\n",
    "!pip show pangaeapy\n",
    "\n",
    "## for details on PANGAEApy see https://pypi.org/project/pangaeapy/ \n",
    "\n",
    "import pangaeapy as pan\n",
    "from pangaeapy.pandataset import PanDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ignore warnings in this script\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=(SettingWithCopyWarning))\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search PAGES_C-PEAT records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all results and combine them in data frame.\n",
    "\n",
    "### define search pattern\n",
    "search_pattern = 'project:label:PAGES_C-PEAT'\n",
    "\n",
    "### basic query to get number of search results\n",
    "query = pan.PanQuery(search_pattern, limit = 500)\n",
    "\n",
    "### create empty data frame\n",
    "df_PAGES = pd.DataFrame()\n",
    "\n",
    "### loop over all results in steps of 500\n",
    "for i in np.arange(0,query.totalcount,500):\n",
    "    \n",
    "    ### store result of individual step in qs\n",
    "    qs = pan.PanQuery(search_pattern, limit = 500, offset=i)\n",
    "    \n",
    "    ### convert qs result with 500 entries to data frame df_qs\n",
    "    df_qs = pd.DataFrame(qs.result)\n",
    "    \n",
    "    ### concatenate all individual df_qs into one data frame named query_results_all\n",
    "    df_PAGES = pd.concat([df_PAGES,df_qs],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loop over all entries in df and get metadata for each entry\n",
    "\n",
    "# NOTE: As a safety precaution, the number of metadata requests is limited for a specific time period. \n",
    "# _Received too many (metadata) requests error (429)...waiting 30s -_\n",
    "\n",
    "# If you have larger requests, prepare to wait or use a different tool e.g. OAI-PMH (https://wiki.pangaea.de/wiki/OAI-PMH).\n",
    "\n",
    "for ind,value in df_PAGES['URI'].items():    \n",
    "    ## use PanDataSet to get metadata \n",
    "    ds = PanDataSet(value, include_data=False)\n",
    "    # print(ind, ds.doi)\n",
    "    ## put metadata into df in new columns\n",
    "    df_PAGES.loc[ind,'Citation'] = ds.citation\n",
    "    df_PAGES.loc[ind,'DOI'] = ds.uri\n",
    "    df_PAGES.loc[ind,'PANGAEA ID'] = ds.id\n",
    "    df_PAGES.loc[ind,'Title'] = ds.title\n",
    "     \n",
    "    if ds.events:\n",
    "        df_PAGES.loc[ind,'event label'] = \"; \".join([x.label for x in ds.events])\n",
    "\n",
    "df_PAGES_citation = df_PAGES[['Citation','DOI']]\n",
    "\n",
    "### write citations as txt file\n",
    "date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "df_PAGES_citation.to_csv('citations_PAGES_'+date_today+'.txt',index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split metadata by type\n",
    "get actual data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PAGES_age  = df_PAGES[df_PAGES['Title'].str.startswith('Age determination')]\n",
    "df_PAGES_geochem  = df_PAGES[df_PAGES['Title'].str.startswith('Geochemistry')]\n",
    "df_PAGES_cal  = df_PAGES[df_PAGES['Title'].str.startswith('Calibrated ages')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort PANGAEA id numbers by event\n",
    "df_events_list = df_PAGES['event label'].unique()\n",
    "df_events = pd.DataFrame(list(zip(df_events_list)), columns = ['event'])\n",
    "# df_events_list\n",
    "df_events.insert(loc=1,column='age',value=np.nan)\n",
    "df_events.insert(loc=2,column='cal_age',value=np.nan)\n",
    "df_events.insert(loc=3,column='geochem',value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,value in df_PAGES['event label'].items():\n",
    "    for ind2,value2 in df_events['event'].items():\n",
    "        if value==value2:\n",
    "            if df_PAGES.loc[ind,'Title'].startswith('Calibrated ages'):\n",
    "                df_events.loc[ind2,'cal_age'] = df_PAGES.loc[ind,'PANGAEA ID']\n",
    "            elif  df_PAGES.loc[ind,'Title'].startswith('Geochemistry'):\n",
    "                df_events.loc[ind2,'geochem'] = df_PAGES.loc[ind,'PANGAEA ID']\n",
    "            elif  df_PAGES.loc[ind,'Title'].startswith('Age determination'):\n",
    "                df_events.loc[ind2,'age'] = df_PAGES.loc[ind,'PANGAEA ID']\n",
    "               \n",
    "df_events.to_csv('dataset_IDs_by_events_'+date_today+'.txt',index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data of individual types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translate short parameters names to long names including unit\n",
    "def get_long_parameters(ds):\n",
    "    \"\"\"Translate short parameters names to long names including unit\n",
    "\n",
    "    Args:\n",
    "        ds (PANGAEA dataset): PANGAEA dataset\n",
    "    \"\"\"\n",
    "    ds.data.columns =  [f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Practical functions for complicated datasets**  \n",
    "* double parameter\n",
    "* method as comment  \n",
    "\n",
    "Example dataset: https://doi.pangaea.de/10.1594/PANGAEA.890478  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to find duplicate column names\n",
    "def find_duplicates(col_names):\n",
    "    name_counts = Counter(col_names)\n",
    "    duplicates = [name for name, count in name_counts.items() if count > 1]\n",
    "    return duplicates, bool(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create general parameter long names with unit and method\n",
    "def generate_general_column_name(param):\n",
    "    base_name = f'{param.name} [{param.unit}]' if param.unit else param.name \n",
    "\n",
    "    if param.method:\n",
    "        base_name += f', method:{param.method.name}'\n",
    "\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions to rename duplicate column names so they are all individual within the dataset\n",
    "def generate_unique_column_name(param):\n",
    "    base_name = f'{param.name} [{param.unit}]' if param.unit else param.name\n",
    "    \n",
    "    if param.method:\n",
    "        base_name += f', method:{param.method.name}'\n",
    "    \n",
    "    if param.comment:\n",
    "        return f'{base_name}, comment:{param.comment}'\n",
    "    else:\n",
    "        return f'{base_name}, col nr:{param.colno}'\n",
    "\n",
    "def make_unique_column_names(ds, same_param_name):\n",
    "    col_names = []\n",
    "    for param in ds.params.values():\n",
    "        name = generate_general_column_name(param)\n",
    "        if name in same_param_name:\n",
    "            name = generate_unique_column_name(param)\n",
    "        col_names.append(name)\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geochemistry datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PAGES_geochem.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create one data frame for all datasets\n",
    "df_PAGES_geochem_data = pd.DataFrame()\n",
    "\n",
    "### loop over all datasets in df_PAGES_geochem\n",
    "for ind,value in df_PAGES_geochem['URI'].items():\n",
    "    \n",
    "    ## use PanDataSet to get metadata and data and put them into 2 diferent dataframes\n",
    "    ds = PanDataSet(value)\n",
    "\n",
    "    print(ind, ds.doi)\n",
    "    \n",
    "    ### Translate default short parameter names to long parameter names, add unit and method if available, check if all column names are individuals\n",
    "    col_names = []\n",
    "    for param in ds.params.values():\n",
    "        col_name = generate_general_column_name(param)\n",
    "        col_names.append(col_name)\n",
    "\n",
    "    ### find duplicate column names make them individual column names\n",
    "    same_param_name, double_name = find_duplicates(col_names)\n",
    "\n",
    "    if double_name:\n",
    "        col_names = make_unique_column_names(ds, set(same_param_name))\n",
    "    \n",
    "    ### rename columns because python cannot handle duplicate column names within dataframe\n",
    "    ds.data.columns =  col_names\n",
    "    \n",
    "    ### create new data dataframe for each query result \n",
    "    df_data = pd.DataFrame()\n",
    "    df_data = ds.data\n",
    "    df_data['DOI'] = ds.doi\n",
    "    if ds.events:\n",
    "        df_data['Event label'] = \"; \".join([x.label for x in ds.events])\n",
    "        \n",
    "    ### combine all datasats into one dataframe\n",
    "    df_PAGES_geochem_data = pd.concat([df_PAGES_geochem_data,df_data], ignore_index=True)\n",
    "\n",
    "### save data\n",
    "# df_PAGES_geochem_data.to_csv('../Data/geochem_data_all_'+date_today+'.txt', sep='\\t', encoding='utf-8', index=False)\n",
    "df_PAGES_geochem_data.to_csv('geochem_data_all_'+date_today+'.txt', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrated Ages datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PAGES_cal.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create one data frame for all datasets\n",
    "df_PAGES_cal_data = pd.DataFrame()\n",
    "\n",
    "### loop over all datasets in df_PAGES_geochem\n",
    "for ind,value in df_PAGES_cal['URI'].items():\n",
    "    \n",
    "    ## use PanDataSet to get metadata and data and put them into 2 diferent dataframes\n",
    "    ds = PanDataSet(value)\n",
    "\n",
    "    print(ind, ds.doi)\n",
    "    \n",
    "    ### Translate default short parameter names to long parameter names, add unit and method if available, check if all column names are individuals\n",
    "    col_names = []\n",
    "    for param in ds.params.values():\n",
    "        col_name = generate_general_column_name(param)\n",
    "        col_names.append(col_name)\n",
    "\n",
    "    ### find duplicate column names make them individual column names\n",
    "    same_param_name, double_name = find_duplicates(col_names)\n",
    "\n",
    "    if double_name:\n",
    "        col_names = make_unique_column_names(ds, set(same_param_name))\n",
    "    \n",
    "    ### rename columns because python cannot handle duplicate column names within dataframe\n",
    "    ds.data.columns =  col_names\n",
    "    \n",
    "    ### create new data dataframe for each query result \n",
    "    df_data = pd.DataFrame()\n",
    "    df_data = ds.data\n",
    "    df_data['DOI'] = ds.doi\n",
    "    if ds.events:\n",
    "        df_data['Event label'] = \"; \".join([x.label for x in ds.events])\n",
    "        \n",
    "    ### combine all datasats into one dataframe\n",
    "    df_PAGES_cal_data = pd.concat([df_PAGES_cal_data,df_data], ignore_index=True)\n",
    "\n",
    "### save data\n",
    "df_PAGES_cal_data.to_csv('cal_age_data_all_'+date_today+'.txt', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangaeapy]",
   "language": "python",
   "name": "conda-env-pangaeapy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
